/**
 * LLM Provider ç«¯åˆ°ç«¯æµ‹è¯•
 * ä½¿ç”¨çœŸå®çš„ API è°ƒç”¨æ¥æµ‹è¯• LLM æä¾›è€…
 *
 * éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
 * - LLM_API_KEY: API å¯†é’¥
 * - LLM_BASE_URL: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼Œç”¨äº OpenAI å…¼å®¹çš„æœåŠ¡ï¼‰
 * - LLM_MODEL: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
 * - LLM_PROVIDER: æä¾›è€…ç±»å‹ï¼ˆanthropic | openai | qwen | deepseekï¼‰ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
 */

import { describe, it, expect, beforeAll } from 'vitest'
import { LLMProvider, LLMConfig, LLMMessage } from '../llm'

// ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
function getConfigFromEnv(): LLMConfig | null {
  // æ”¯æŒå¤šç§ç¯å¢ƒå˜é‡åç§°
  const apiKey =
    process.env.LLM_API_KEY ||
    process.env.DEEPSEEK_API_KEY ||
    process.env.OPENAI_API_KEY ||
    process.env.ANTHROPIC_API_KEY
  let baseURL = process.env.LLM_BASE_URL || process.env.DEEPSEEK_BASE_URL
  let model = process.env.LLM_MODEL || process.env.DEEPSEEK_MODEL
  let provider = process.env.LLM_PROVIDER as LLMConfig['provider'] | undefined

  if (!apiKey) {
    return null
  }

  // æ ¹æ®ä½¿ç”¨çš„ç¯å¢ƒå˜é‡è‡ªåŠ¨è®¾ç½® provider å’Œ baseURL
  if (!provider) {
    if (process.env.DEEPSEEK_API_KEY) {
      provider = 'deepseek'
      // è‡ªåŠ¨è®¾ç½® DeepSeek çš„ baseURL
      if (!baseURL) {
        baseURL = 'https://api.deepseek.com/v1'
      }
      // è‡ªåŠ¨è®¾ç½® DeepSeek çš„é»˜è®¤æ¨¡å‹
      if (!model) {
        model = 'deepseek-chat'
      }
    }
  }

  // æ ¹æ® provider è‡ªåŠ¨è®¾ç½®é»˜è®¤ baseURL å’Œ model
  if (provider === 'deepseek') {
    if (!baseURL) {
      baseURL = 'https://api.deepseek.com/v1'
    }
    if (!model) {
      model = 'deepseek-chat'
    }
  } else if (provider === 'qwen' && !baseURL) {
    baseURL = 'https://dashscope.aliyuncs.com/compatible-mode/v1'
  }

  // è‡ªåŠ¨æ£€æµ‹æä¾›è€…
  if (!provider) {
    if (baseURL?.includes('deepseek')) {
      provider = 'deepseek'
    } else if (baseURL?.includes('dashscope') || baseURL?.includes('aliyun')) {
      provider = 'qwen'
    } else if (baseURL?.includes('anthropic') || apiKey.startsWith('sk-ant-')) {
      provider = 'anthropic'
    } else {
      provider = 'openai' // é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼
    }
  }

  return {
    provider,
    apiKey,
    baseURL,
    model,
  }
}

describe('LLMProvider E2E Tests', () => {
  let config: LLMConfig | null

  beforeAll(() => {
    config = getConfigFromEnv()
  })

  describe('çœŸå® API è°ƒç”¨', () => {
    it('åº”è¯¥æˆåŠŸè°ƒç”¨ LLM å¹¶è·å–å“åº”', async () => {
      if (!config) {
        console.log('âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæœªè®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡')
        return
      }

      console.log(`\nğŸ”§ æµ‹è¯•é…ç½®:`)
      console.log(`   Provider: ${config.provider}`)
      console.log(`   Model: ${config.model || '(é»˜è®¤)'}`)
      console.log(`   Base URL: ${config.baseURL || '(é»˜è®¤)'}`)

      const provider = new LLMProvider(config)

      const messages: LLMMessage[] = [
        { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„åŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å›ç­”ï¼Œå›å¤æ§åˆ¶åœ¨20å­—ä»¥å†…ã€‚' },
        { role: 'user', content: '1+1ç­‰äºå‡ ï¼Ÿ' },
      ]

      console.log(`\nğŸ“¤ å‘é€æ¶ˆæ¯: "${messages[1].content}"`)

      const startTime = Date.now()
      const response = await provider.call(messages)
      const duration = Date.now() - startTime

      console.log(`\nğŸ“¥ æ”¶åˆ°å“åº” (${duration}ms):`)
      console.log(`   å†…å®¹: "${response.content}"`)
      if (response.usage) {
        console.log(
          `   Token ä½¿ç”¨: è¾“å…¥=${response.usage.inputTokens}, è¾“å‡º=${response.usage.outputTokens}, æ€»è®¡=${response.usage.totalTokens}`,
        )
      }

      // éªŒè¯å“åº”
      expect(response.content).toBeTruthy()
      expect(response.content.length).toBeGreaterThan(0)
      expect(response.content).toMatch(/2|äºŒ|ä¸¤/) // åº”è¯¥åŒ…å«æ­£ç¡®ç­”æ¡ˆ
    }, 30000) // 30ç§’è¶…æ—¶

    it('åº”è¯¥æ­£ç¡®å¤„ç†å¤šè½®å¯¹è¯', async () => {
      if (!config) {
        console.log('âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæœªè®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡')
        return
      }

      const provider = new LLMProvider(config)

      const messages: LLMMessage[] = [
        { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªæ•°å­¦åŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å›ç­”ï¼Œå›å¤ç®€æ´ã€‚' },
        { role: 'user', content: 'æˆ‘æœ‰3ä¸ªè‹¹æœ' },
        { role: 'assistant', content: 'å¥½çš„ï¼Œä½ æœ‰3ä¸ªè‹¹æœã€‚' },
        { role: 'user', content: 'æˆ‘åˆä¹°äº†2ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ' },
      ]

      console.log(`\nğŸ“¤ å¤šè½®å¯¹è¯æµ‹è¯•...`)

      const response = await provider.call(messages)

      console.log(`ğŸ“¥ å“åº”: "${response.content}"`)

      expect(response.content).toBeTruthy()
      expect(response.content).toMatch(/5|äº”/) // åº”è¯¥åŒ…å«æ­£ç¡®ç­”æ¡ˆ
    }, 30000)

    it('åº”è¯¥æ­£ç¡®å¤„ç†ä¸­æ–‡å†…å®¹', async () => {
      if (!config) {
        console.log('âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæœªè®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡')
        return
      }

      const provider = new LLMProvider(config)

      const messages: LLMMessage[] = [{ role: 'user', content: 'ç”¨ä¸€ä¸ªæˆè¯­å½¢å®¹"éå¸¸å¼€å¿ƒ"' }]

      console.log(`\nğŸ“¤ ä¸­æ–‡æµ‹è¯•: "${messages[0].content}"`)

      const response = await provider.call(messages)

      console.log(`ğŸ“¥ å“åº”: "${response.content}"`)

      expect(response.content).toBeTruthy()
      // ä¸­æ–‡æˆè¯­é€šå¸¸æ˜¯4ä¸ªå­—
      expect(response.content.length).toBeGreaterThan(0)
    }, 30000)
  })

  describe('é”™è¯¯å¤„ç†', () => {
    it('åº”è¯¥å¤„ç†æ— æ•ˆçš„ API Key', async () => {
      const invalidConfig: LLMConfig = {
        provider: 'openai',
        apiKey: 'invalid-key-12345',
        baseURL: config?.baseURL,
        model: config?.model,
      }

      const provider = new LLMProvider(invalidConfig)

      await expect(provider.call([{ role: 'user', content: 'æµ‹è¯•' }])).rejects.toThrow()
    }, 30000)
  })
})

/**
 * è¿è¡Œè¯´æ˜ï¼š
 *
 * 1. è®¾ç½®ç¯å¢ƒå˜é‡åè¿è¡Œï¼š
 *    LLM_API_KEY=your-key LLM_BASE_URL=https://api.deepseek.com/v1 LLM_MODEL=deepseek-chat yarn test llm.e2e
 *
 * 2. æˆ–è€…åˆ›å»º .env æ–‡ä»¶ï¼ˆéœ€è¦ dotenv æ”¯æŒï¼‰
 *
 * 3. æ”¯æŒçš„æä¾›è€…ï¼š
 *    - anthropic: Claude ç³»åˆ—æ¨¡å‹
 *    - openai: GPT ç³»åˆ—æ¨¡å‹
 *    - qwen: é€šä¹‰åƒé—®ï¼ˆé˜¿é‡Œäº‘ï¼‰
 *    - deepseek: DeepSeek æ¨¡å‹
 */
